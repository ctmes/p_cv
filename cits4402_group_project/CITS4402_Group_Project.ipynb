{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CITS4402: Automatic Calibration of a Holographic Acquisition Rig\n",
        "#### Group members: Ryan Christie (22500934), Thomas Fell-Smith (23209005), Colin Melville (23170781)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### Timeline\n",
        "* **Week 7 to Week 12**: Work on your project. The facilitator will be available on Wednesdays (4 pm \n",
        "to 6 pm) for any query in room G.04, or can be contacted through email, nasir.ahmad@uwa.edu.au.\n",
        "\n",
        "* **Week 12**: Presentation and Demonstration of the project will be during lab timings on Wednesday \n",
        "and Thursday of Week 12. Timing for individual groups will be announced at a later stage.\n",
        "\n",
        "* **23 May 2023, 11:59 pm**: Final deadline to submit your project on LMS. One submission per group.\n",
        "\n"
      ],
      "metadata": {
        "id": "rXeKs_dlj9IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Rough Detection\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The first step is to detect the candidate targets in the images. To this end, use the method described in section 1.2 of the attached thesis extracts. \n",
        "\n",
        "You are free to use existing functions of matlab or python to conduct the connected components analysis. All the required measurements can be extracted from the binary images by using the blobanalysis class in the matlab vision toolbox \n",
        "https://au.mathworks.com/help/vision/ref/vision.blobanalysis-system-object.html. \n",
        "\n",
        "In python, you can use skimage.measure.label:\n",
        "* https://scikit\u0002image.org/docs/stable/api/skimage.measure.html#skimage.measure.label \n",
        "\n",
        "and skimage.measure.regionprops from the skimage package:\n",
        "* https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops \n",
        "\n",
        "for the same purpose. \n",
        "\n",
        "Your code should then display the input image, along with the approximate position of all targetsâ€™ points (you can indicate the position of each point with squares drawn around them, by using a binary mask or by marking them with a dot). Each target should have six and only six points. As an image might contain multiple targets you can use different colours to differentiate the individual targets when you display the results."
      ],
      "metadata": {
        "id": "tm4nCSOTkSew"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCKmiLJVmMU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Target Analysis and Refinement\n",
        "\n",
        "---\n",
        "\n",
        "The second step in the project is to implement the analysis and refinement methods described in section 1.3 and 1.4 of the attached thesis extract.\n",
        "Your code should then display the refined centroid position of each target's points. Each target should be labeled with the string corresponding to the target color arrangement, as in figure 2 of the thesis extract. "
      ],
      "metadata": {
        "id": "IldLjSWakXou"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Qj04vYfmMyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Camera Alignments\n",
        "\n",
        "---\n",
        "\n",
        "The last step, is to use the points you measured in the two previous steps to find the position of the targets and cameras in the rig, as described in section 1.5 of the attached thesis extract. You are free to use existing functions for the PnP problem, which are estworldpose for Matlab:\n",
        "* https://www.mathworks.com/help/vision/ref/estworldpose.html\n",
        "\n",
        "and solvePnP for OpenCV:\n",
        "* https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html.\n",
        "\n",
        "It is your responsibility to implement \n",
        "the global optimization function based on what you have been presented in the lectures. Your code should then display the 3D position of the targets' points and the cameras as displayed in figure 3 of the attached thesis extract. In matlab, you can do that with the plot3 function:\n",
        "\n",
        "* https://www.mathworks.com/help/matlab/ref/plot3.html. \n",
        "\n",
        "In python, you can use the mplot3d toolkit:\n",
        "* https://matplotlib.org/stable/tutorials/toolkits/mplot3d.html."
      ],
      "metadata": {
        "id": "6o31TrmLkbcW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7bxWMVQAmNaw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}